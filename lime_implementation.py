
"""lime_implementation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v6Nd--dq58Fj__f59H8Fgy9m00zzZ_yT
"""
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

class SimpleLIME:
    def __init__(self, X_train, feature_names, kernel_width=1.0, n_samples=5000):
        """
        Paramètres
        ----------
        X_train : array, shape (n_samples, n_features)
            Données d’entraînement
        feature_names : list
            Noms des features
        kernel_width : float
            Paramètre du noyau RBF (sigma)
        n_samples : int
            Nombre de perturbations à générer
        """
        self.X_train = X_train
        self.feature_names = feature_names
        self.kernel_width = kernel_width
        self.n_samples = n_samples

        # Calculer mean et std pour les perturbations
        self.data_mean = np.mean(X_train, axis=0)
        self.data_std = np.std(X_train, axis=0)

    def explain_instance(self, instance, predict_fn, num_features=None):
        """
        Explique une prédiction pour une instance donnée.

        Paramètres
        ----------
        instance : array, shape (n_features,)
            L’instance x* à expliquer
        predict_fn : callable
            Fonction de prédiction du modèle h
        num_features : int
            Nombre de features à retourner

        Retourne
        -------
        explanation : dict
            {'coefficients': dict, 'intercept': float, 'r2_score': float}
        """
        # Votre implémentation des 4 étapes ici

        # ETAPE 1 : CHOISIR L'INSTANCE*
        x_etoile = instance

        # ÉTAPE 2: Générer des voisins autour de x*
        neighbors = self._generate_neighborhood(x_etoile)

        # ÉTAPE 3: Obtenir les prédictions et calculer les poids sur les voisins
        predictions = self._predict_on_neighbors(neighbors, predict_fn)
        weights = self._compute_weights(x_etoile, neighbors)

        # ÉTAPE 4a: Calculer les poids de proximité
        weights = self._compute_weights(x_etoile, neighbors)

        # Étape 4b
        coefficients, intercept, r2 = self._fit_local_linear_model(
            neighbors,
            predictions,
            weights
        )

        # Formater les résultats
        #associer chaque nom de caractéristique (self.feature_names) à son coefficient linéaire correspondant (coefficients)
        # pour obtenir un mapping {'age': 0.32, 'salary': -1.1, ...}.
        feature_importance = dict(zip(self.feature_names, coefficients))

        # Trier par importance absolue
        if num_features:
            sorted_features = sorted(
                feature_importance.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:num_features]

            feature_importance = dict(sorted_features)

        return {
            'coefficients': feature_importance,
            'intercept': intercept,  #la valeur de sortie du modèle quand toutes les features valent zéro.
            'r2_score': r2,
            'instance': instance
        }

        pass

    def _generate_neighborhood(self, instance):
        """
        Genere N voisins autour de l'instance par perturbations gaussiennes.

        Formule du cours:
        z_i = x* + epsilon_i
        ou epsilon_i ~ N(0, sigma^2_pert)

        Retourne:
        --------
        neighbors : array, shape (n_samples, n_features)
            Les voisins generes
        """
        n_features = len(instance)
        neighbors = np.zeros((self.n_samples, n_features))

        # Paramètre d'échelle pour les perturbations
        scale = 0.1 * self.data_std

        for i in range(self.n_samples):
            # Générer epsilon_i ~ N(0, sigma^2_pert)
            epsilon_i = np.random.normal(loc=0, scale=scale, size=n_features)

            # z_i = x* + epsilon_i
            neighbors[i] = instance + epsilon_i

        # Remplacer le premier voisin par l'instance originale x*
        # (important pour avoir un point avec poids maximal)
        neighbors[0] = instance

        return neighbors

    def _predict_on_neighbors(self, neighbors, predict_fn):
        """
        Utilise le modele h pour predire sur tous les voisins.

        Formule du cours:
        y_hat_i = h(z_i) pour chaque voisin z_i

        Retourne:
        --------
        predictions : array, shape (n_samples,)
            Les predictions du modele sur les voisins
        """
        # Appliquer la fonction de prédiction sur tous les voisins
        predictions = predict_fn(neighbors)

        return predictions

    def _compute_weights(self, instance, neighbors):
        # on calcule les poids pour que lime ne traite pas chaque voivins de la meme maniere pour que l'explication soit locale et precise
        """
        Calcule les poids de proximite avec le noyau RBF.

        Formule du cours:
        pi(z) = exp(-d^2(x*, z) / (2 * sigma^2))
        ou d(x*, z) = ||x* - z||_2 (distance euclidienne)

        Paramètres
        ----------
        instance : array, shape (n_features,)
            L'instance x* à expliquer
        neighbors : array, shape (n_samples, n_features)
            Les voisins générés

        Retourne
        --------
        weights : array, shape (n_samples,)
            Les poids de chaque voisin
        """
        # Calculer toutes les distances euclidiennes en une fois
        distances = np.linalg.norm(neighbors - instance, axis=1)

        # Appliquer le noyau RBF
        # pi(z_i) = exp(-d^2 / (2*sigma^2))
        weights = np.exp(-distances**2 / (2 * self.kernel_width**2))

        return weights

    def _fit_local_linear_model(self, neighbors, predictions, weights):
        """
        Entraine une regression lineaire ponderee.

        Formule du cours:
        g* = argmin_g sum(pi(z_i) * (h(z_i) - g(z_i))^2)
        avec g(z) = w_0 + w_1*z_1 + w_2*z_2 + ... + w_d*z_d

        Paramètres
        ----------
        neighbors : array, shape (n_samples, n_features)
            Les voisins z_i
        predictions : array, shape (n_samples,)
            Les prédictions h(z_i) du modèle complexe
        weights : array, shape (n_samples,)
            Les poids pi(z_i) de proximité

        Retourne
        --------
        coefficients : array, shape (n_features,?)
            Les coefficients w_j du modele lineaire
        intercept : float
            L'intercept w_0
        r2_score : float
            Score de fidelite locale (R^2)
        """
        # Créer le modèle de régression linéaire
        model = LinearRegression()

        # Entraîner le modèle avec les poids de proximité
        # sample_weight indique l'importance de chaque échantillon
        # g* = argmin_g sum(pi(z_i) * (h(z_i) - g(z_i))^2)
        # est utilisée implicitement dans cette ligne
        model.fit(neighbors, predictions, sample_weight=weights)

        # Extraire les coefficients et l'intercept
        coefficients = model.coef_
        intercept = model.intercept_

        # Calculer les prédictions du modèle local g
        predictions_local = model.predict(neighbors)

        # Calculer le R^2 pondéré pour mesurer la fidélité
        r2 = r2_score(predictions, predictions_local, sample_weight=weights)

        return coefficients, intercept, r2
